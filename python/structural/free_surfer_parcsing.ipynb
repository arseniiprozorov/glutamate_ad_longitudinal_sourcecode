{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parsed aseg.stats file\n",
      "  Rows: 45\n",
      "  Columns: Index, SegId, NVoxels, Volume_mm3, StructName, normMean, normStdDev, normMin, normMax, normRange, ICV\n",
      "  ICV (eTIV): 1349509.5 mm³\n",
      "\n",
      "✓ Saved to: C:\\Users\\okkam\\Desktop\\aseg_parsed.csv\n",
      "\n",
      "First few rows:\n",
      "   Index  SegId  NVoxels  Volume_mm3                    StructName  normMean  \\\n",
      "0      1      4    10556     10709.2        Left-Lateral-Ventricle   21.1470   \n",
      "1      2      5      302       319.3             Left-Inf-Lat-Vent   36.0099   \n",
      "2      3      7    12870     13558.6  Left-Cerebellum-White-Matter   82.4620   \n",
      "3      4      8    40401     39555.7        Left-Cerebellum-Cortex   54.1926   \n",
      "4      5     10     6338      6206.1          Left-Thalamus-Proper   88.7248   \n",
      "\n",
      "   normStdDev  normMin  normMax  normRange           ICV  \n",
      "0     11.6668      0.0     75.0       75.0  1.349510e+06  \n",
      "1     14.2427      6.0     77.0       71.0  1.349510e+06  \n",
      "2      7.4717     13.0    109.0       96.0  1.349510e+06  \n",
      "3     13.5459      0.0    118.0      118.0  1.349510e+06  \n",
      "4      9.7764     45.0    118.0       73.0  1.349510e+06  \n"
     ]
    }
   ],
   "source": [
    "### Parcsing Aseg.stats file and saving to CSV\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the aseg.stats file\n",
    "stats_file = Path(r\"C:\\Users\\okkam\\Desktop\\aseg.stats\")\n",
    "csv_output = Path(r\"C:\\Users\\okkam\\Desktop\\aseg_parsed.csv\")\n",
    "\n",
    "# Read the file and extract the data table\n",
    "with open(stats_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract ICV (eTIV) from header\n",
    "icv = None\n",
    "for line in lines:\n",
    "    if 'EstimatedTotalIntraCranialVol' in line and line.startswith('# Measure'):\n",
    "        parts = line.split(',')\n",
    "        if len(parts) >= 4:\n",
    "            icv = float(parts[3].strip())\n",
    "            break\n",
    "\n",
    "# Find the line with column headers\n",
    "header_line_idx = None\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('# ColHeaders'):\n",
    "        header_line_idx = i\n",
    "        break\n",
    "\n",
    "# Extract column names from the header line\n",
    "header_line = lines[header_line_idx].strip()\n",
    "columns = header_line.replace('# ColHeaders ', '').split()\n",
    "\n",
    "# Find where data starts (after the header line)\n",
    "data_start_idx = header_line_idx + 1\n",
    "\n",
    "# Parse the data rows\n",
    "data_rows = []\n",
    "for line in lines[data_start_idx:]:\n",
    "    if line.strip() and not line.startswith('#'):\n",
    "        values = line.strip().split()\n",
    "        if len(values) == len(columns):\n",
    "            data_rows.append(values)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "# Add ICV column\n",
    "if icv is not None:\n",
    "    df['ICV'] = icv\n",
    "\n",
    "# Convert numeric columns to appropriate types\n",
    "numeric_cols = ['Index', 'SegId', 'NVoxels', 'Volume_mm3', 'normMean', 'normStdDev', 'normMin', 'normMax', 'normRange', 'ICV']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(csv_output, index=False)\n",
    "\n",
    "print(f\"✓ Parsed aseg.stats file\")\n",
    "print(f\"  Rows: {len(df)}\")\n",
    "print(f\"  Columns: {', '.join(df.columns)}\")\n",
    "print(f\"  ICV (eTIV): {icv:.1f} mm³\")\n",
    "print(f\"\\n✓ Saved to: {csv_output}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 aseg.stats files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing aseg.stats files: 100%|██████████| 65/65 [00:04<00:00, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete!\n",
      "  Successfully processed: 65\n",
      "  Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Batch processing multiple aseg.stats files in a folder\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to T1 folder with all participants\n",
    "t1_folder = Path(r\"D:\\02-Raw_data-anat\\longitudinal_freesurfer_149\\longi_output\\T2\")\n",
    "\n",
    "# Find all aseg.stats files\n",
    "aseg_files = list(t1_folder.rglob(\"aseg.stats\"))\n",
    "print(f\"Found {len(aseg_files)} aseg.stats files\\n\")\n",
    "\n",
    "# Function to parse aseg.stats file\n",
    "def parse_aseg_stats(stats_file):\n",
    "    \"\"\"Parse aseg.stats file and extract data\"\"\"\n",
    "    with open(stats_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract ICV (eTIV) from header\n",
    "    icv = None\n",
    "    for line in lines:\n",
    "        if 'EstimatedTotalIntraCranialVol' in line and line.startswith('# Measure'):\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 4:\n",
    "                icv = float(parts[3].strip())\n",
    "                break\n",
    "    \n",
    "    # Find the line with column headers\n",
    "    header_line_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('# ColHeaders'):\n",
    "            header_line_idx = i\n",
    "            break\n",
    "    \n",
    "    if header_line_idx is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract column names\n",
    "    header_line = lines[header_line_idx].strip()\n",
    "    columns = header_line.replace('# ColHeaders ', '').split()\n",
    "    \n",
    "    # Find where data starts\n",
    "    data_start_idx = header_line_idx + 1\n",
    "    \n",
    "    # Parse the data rows\n",
    "    data_rows = []\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            values = line.strip().split()\n",
    "            if len(values) == len(columns):\n",
    "                data_rows.append(values)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "    \n",
    "    # Add ICV column\n",
    "    if icv is not None:\n",
    "        df['ICV'] = icv\n",
    "    \n",
    "    # Convert numeric columns to appropriate types\n",
    "    numeric_cols = ['Index', 'SegId', 'NVoxels', 'Volume_mm3', 'normMean', 'normStdDev', 'normMin', 'normMax', 'normRange', 'ICV']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df, icv\n",
    "\n",
    "# Process all aseg.stats files\n",
    "processed = 0\n",
    "errors = 0\n",
    "\n",
    "for stats_file in tqdm(aseg_files, desc=\"Processing aseg.stats files\"):\n",
    "    try:\n",
    "        df, icv = parse_aseg_stats(stats_file)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Save CSV in the same stats folder\n",
    "            csv_output = stats_file.parent / \"aseg_parsed.csv\"\n",
    "            df.to_csv(csv_output, index=False)\n",
    "            processed += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "            print(f\"✗ Failed to parse: {stats_file}\")\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"✗ Error processing {stats_file}: {e}\")\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"  Successfully processed: {processed}\")\n",
    "print(f\"  Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741f6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing lh.aparc.stats...\n",
      "Parsing rh.aparc.stats...\n",
      "✓ Saved lh: C:\\Users\\okkam\\Desktop\\lh.aparc_parsed.csv\n",
      "  Regions: 34\n",
      "  Columns: StructName, NumVert, SurfArea, GrayVol, ThickAvg, ThickStd, MeanCurv, GausCurv, FoldInd, CurvInd...\n",
      "✓ Saved rh: C:\\Users\\okkam\\Desktop\\rh.aparc_parsed.csv\n",
      "  Regions: 34\n",
      "  Columns: StructName, NumVert, SurfArea, GrayVol, ThickAvg, ThickStd, MeanCurv, GausCurv, FoldInd, CurvInd...\n",
      "\n",
      "✓ Done! Both aparc files parsed and saved.\n"
     ]
    }
   ],
   "source": [
    "##### aparc processing\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Files to parse\n",
    "lh_file = Path(r\"C:\\Users\\okkam\\Desktop\\lh.aparc.stats\")\n",
    "rh_file = Path(r\"C:\\Users\\okkam\\Desktop\\rh.aparc.stats\")\n",
    "\n",
    "def parse_aparc_stats(stats_file):\n",
    "    \"\"\"Parse aparc.stats file and extract header measures and region data\"\"\"\n",
    "    with open(stats_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract header measures\n",
    "    measures = {}\n",
    "    for line in lines:\n",
    "        if line.startswith('# Measure'):\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 4:\n",
    "                measure_name = parts[0].replace('# Measure ', '').strip()\n",
    "                measure_value = parts[3].strip()\n",
    "                try:\n",
    "                    measures[measure_name] = float(measure_value)\n",
    "                except:\n",
    "                    measures[measure_name] = measure_value\n",
    "    \n",
    "    # Find the line with column headers\n",
    "    col_header_line_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('# ColHeaders'):\n",
    "            col_header_line_idx = i\n",
    "            break\n",
    "    \n",
    "    if col_header_line_idx is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract column names\n",
    "    header_line = lines[col_header_line_idx].strip()\n",
    "    columns = header_line.replace('# ColHeaders ', '').split()\n",
    "    \n",
    "    # Find where data starts\n",
    "    data_start_idx = col_header_line_idx + 1\n",
    "    \n",
    "    # Parse the data rows\n",
    "    data_rows = []\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            values = line.strip().split()\n",
    "            if len(values) >= len(columns):\n",
    "                # Handle structure name which might have spaces (take first column as name)\n",
    "                struct_name = values[0]\n",
    "                data_values = [struct_name] + values[1:len(columns)]\n",
    "                data_rows.append(data_values)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['NumVert', 'SurfArea', 'GrayVol', 'ThickAvg', 'ThickStd', 'MeanCurv', 'GausCurv', 'FoldInd', 'CurvInd']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Add key measures as columns\n",
    "    for measure_name, measure_value in measures.items():\n",
    "        if isinstance(measure_value, (int, float)):\n",
    "            df[measure_name] = measure_value\n",
    "    \n",
    "    return df, measures\n",
    "\n",
    "# Parse both files\n",
    "print(\"Parsing lh.aparc.stats...\")\n",
    "lh_df, lh_measures = parse_aparc_stats(lh_file)\n",
    "\n",
    "print(\"Parsing rh.aparc.stats...\")\n",
    "rh_df, rh_measures = parse_aparc_stats(rh_file)\n",
    "\n",
    "if lh_df is not None:\n",
    "    lh_csv = lh_file.parent / \"lh.aparc_parsed.csv\"\n",
    "    lh_df.to_csv(lh_csv, index=False)\n",
    "    print(f\"✓ Saved lh: {lh_csv}\")\n",
    "    print(f\"  Regions: {len(lh_df)}\")\n",
    "    print(f\"  Columns: {', '.join(lh_df.columns.tolist()[:10])}...\")\n",
    "\n",
    "if rh_df is not None:\n",
    "    rh_csv = rh_file.parent / \"rh.aparc_parsed.csv\"\n",
    "    rh_df.to_csv(rh_csv, index=False)\n",
    "    print(f\"✓ Saved rh: {rh_csv}\")\n",
    "    print(f\"  Regions: {len(rh_df)}\")\n",
    "    print(f\"  Columns: {', '.join(rh_df.columns.tolist()[:10])}...\")\n",
    "\n",
    "print(\"\\n✓ Done! Both aparc files parsed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023ef60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 lh.aparc.stats files\n",
      "Found 65 rh.aparc.stats files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing aparc.stats files: 100%|██████████| 130/130 [00:08<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete!\n",
      "  Successfully processed: 130\n",
      "  Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to T1 folder with all participants\n",
    "t1_folder = Path(r\"D:\\02-Raw_data-anat\\longitudinal_freesurfer_149\\longi_output\\T2\")\n",
    "\n",
    "# Find all lh and rh aparc.stats files\n",
    "lh_aparc_files = list(t1_folder.rglob(\"lh.aparc.stats\"))\n",
    "rh_aparc_files = list(t1_folder.rglob(\"rh.aparc.stats\"))\n",
    "\n",
    "print(f\"Found {len(lh_aparc_files)} lh.aparc.stats files\")\n",
    "print(f\"Found {len(rh_aparc_files)} rh.aparc.stats files\\n\")\n",
    "\n",
    "def parse_aparc_stats(stats_file):\n",
    "    \"\"\"Parse aparc.stats file and extract header measures and region data\"\"\"\n",
    "    with open(stats_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract header measures\n",
    "    measures = {}\n",
    "    for line in lines:\n",
    "        if line.startswith('# Measure'):\n",
    "            parts = line.split(',')\n",
    "            if len(parts) >= 4:\n",
    "                measure_name = parts[0].replace('# Measure ', '').strip()\n",
    "                measure_value = parts[3].strip()\n",
    "                try:\n",
    "                    measures[measure_name] = float(measure_value)\n",
    "                except:\n",
    "                    measures[measure_name] = measure_value\n",
    "    \n",
    "    # Find the line with column headers\n",
    "    col_header_line_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('# ColHeaders'):\n",
    "            col_header_line_idx = i\n",
    "            break\n",
    "    \n",
    "    if col_header_line_idx is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract column names\n",
    "    header_line = lines[col_header_line_idx].strip()\n",
    "    columns = header_line.replace('# ColHeaders ', '').split()\n",
    "    \n",
    "    # Find where data starts\n",
    "    data_start_idx = col_header_line_idx + 1\n",
    "    \n",
    "    # Parse the data rows\n",
    "    data_rows = []\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            values = line.strip().split()\n",
    "            if len(values) >= len(columns):\n",
    "                struct_name = values[0]\n",
    "                data_values = [struct_name] + values[1:len(columns)]\n",
    "                data_rows.append(data_values)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['NumVert', 'SurfArea', 'GrayVol', 'ThickAvg', 'ThickStd', 'MeanCurv', 'GausCurv', 'FoldInd', 'CurvInd']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Add key measures as columns\n",
    "    for measure_name, measure_value in measures.items():\n",
    "        if isinstance(measure_value, (int, float)):\n",
    "            df[measure_name] = measure_value\n",
    "    \n",
    "    return df, measures\n",
    "\n",
    "# Process all lh and rh aparc files\n",
    "processed = 0\n",
    "errors = 0\n",
    "all_aparc_files = lh_aparc_files + rh_aparc_files\n",
    "\n",
    "for aparc_file in tqdm(all_aparc_files, desc=\"Processing aparc.stats files\"):\n",
    "    try:\n",
    "        df, measures = parse_aparc_stats(aparc_file)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Determine output filename based on lh or rh\n",
    "            if 'lh.aparc' in aparc_file.name:\n",
    "                csv_output = aparc_file.parent / \"lh.aparc_parsed.csv\"\n",
    "            else:\n",
    "                csv_output = aparc_file.parent / \"rh.aparc_parsed.csv\"\n",
    "            \n",
    "            df.to_csv(csv_output, index=False)\n",
    "            processed += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "            print(f\"✗ Failed to parse: {aparc_file}\")\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"✗ Error processing {aparc_file}: {e}\")\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"  Successfully processed: {processed}\")\n",
    "print(f\"  Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 participants with complete data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|██████████| 65/65 [00:02<00:00, 22.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Combined data saved!\n",
      "  Participants: 65\n",
      "  Total Variables: 116\n",
      "  Output: D:\\02-Raw_data-anat\\longitudinal_freesurfer_149\\longi_output\\T2\\all_participants_freesurfer_data.csv\n",
      "\n",
      "Variable breakdown:\n",
      "  - Base columns (participant_id, visit): 2\n",
      "  - ASEG volumes (45 structures × Volume_mm3 + ICV): 46\n",
      "  - LH APARC volumes (34 regions × GrayVol): 34\n",
      "  - RH APARC volumes (34 regions × GrayVol): 34\n",
      "  - TOTAL: 116\n",
      "\n",
      "Sample columns:\n",
      "['participant_id', 'visit', 'aseg_Left_Lateral_Ventricle_Volume_mm3', 'aseg_ICV', 'aseg_Left_Inf_Lat_Vent_Volume_mm3', 'aseg_Left_Cerebellum_White_Matter_Volume_mm3', 'aseg_Left_Cerebellum_Cortex_Volume_mm3', 'aseg_Left_Thalamus_Proper_Volume_mm3', 'aseg_Left_Caudate_Volume_mm3', 'aseg_Left_Putamen_Volume_mm3', 'aseg_Left_Pallidum_Volume_mm3', 'aseg_3rd_Ventricle_Volume_mm3', 'aseg_4th_Ventricle_Volume_mm3', 'aseg_Brain_Stem_Volume_mm3', 'aseg_Left_Hippocampus_Volume_mm3', 'aseg_Left_Amygdala_Volume_mm3', 'aseg_CSF_Volume_mm3', 'aseg_Left_Accumbens_area_Volume_mm3', 'aseg_Left_VentralDC_Volume_mm3', 'aseg_Left_vessel_Volume_mm3']\n",
      "\n",
      "First few rows:\n",
      "  participant_id    visit  aseg_Left_Lateral_Ventricle_Volume_mm3  \\\n",
      "0        3002498      t06                                 11128.0   \n",
      "1        3002498      t06                                 11128.0   \n",
      "2        3420680  unknown                                 12473.1   \n",
      "\n",
      "       aseg_ICV  aseg_Left_Inf_Lat_Vent_Volume_mm3  \\\n",
      "0  1.349510e+06                              314.9   \n",
      "1  1.349510e+06                              314.9   \n",
      "2  1.505191e+06                              382.3   \n",
      "\n",
      "   aseg_Left_Cerebellum_White_Matter_Volume_mm3  \n",
      "0                                       13217.6  \n",
      "1                                       13217.6  \n",
      "2                                       12159.3  \n",
      "\n",
      "Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Combine all mm2 data into one CSV\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Path to T1 folder\n",
    "t1_folder = Path(r\"D:\\02-Raw_data-anat\\longitudinal_freesurfer_149\\longi_output\\T2\")\n",
    "\n",
    "# Find all participant directories - look for folders containing \"stats\" subfolder with parsed CSVs\n",
    "participant_stats_dirs = list(t1_folder.rglob(\"stats\"))\n",
    "# Filter to only those that have all three required files\n",
    "participant_dirs = []\n",
    "for stats_dir in participant_stats_dirs:\n",
    "    aseg_csv = stats_dir / \"aseg_parsed.csv\"\n",
    "    lh_aparc_csv = stats_dir / \"lh.aparc_parsed.csv\"\n",
    "    rh_aparc_csv = stats_dir / \"rh.aparc_parsed.csv\"\n",
    "    \n",
    "    if aseg_csv.exists() and lh_aparc_csv.exists() and rh_aparc_csv.exists():\n",
    "        participant_dirs.append(stats_dir.parent)\n",
    "\n",
    "print(f\"Found {len(participant_dirs)} participants with complete data\\n\")\n",
    "\n",
    "# Function to extract participant ID and visit from folder name\n",
    "def extract_participant_info(folder_name):\n",
    "    \"\"\"\n",
    "    Extract participant ID and visit from folder name like:\n",
    "    3002498_irm_t04-3192767.long.Base-3192838-1\n",
    "    OR\n",
    "    8193847_irm_t00-3192713.long.8193847_base-3194677-1\n",
    "    Returns: (participant_id, visit)\n",
    "    \"\"\"\n",
    "    # Extract participant ID (first number before underscore)\n",
    "    match_id = re.search(r'^(\\d+)_', folder_name)\n",
    "    participant_id = match_id.group(1) if match_id else \"unknown\"\n",
    "    \n",
    "    # Extract visit (t00, t02, t04, etc.) - look for pattern like _irm_t04\n",
    "    match_visit = re.search(r'_irm_(t\\d+)', folder_name)\n",
    "    visit = match_visit.group(1) if match_visit else \"unknown\"\n",
    "    \n",
    "    return participant_id, visit\n",
    "\n",
    "# Function to flatten aseg data - keep only Volume_mm3 and ICV\n",
    "def flatten_aseg(aseg_df):\n",
    "    \"\"\"Keep only Volume_mm3 (volume in mm3) from aseg data\"\"\"\n",
    "    flattened = {}\n",
    "    \n",
    "    for idx, row in aseg_df.iterrows():\n",
    "        struct_name = row['StructName'].replace('-', '_').replace(' ', '_')\n",
    "        \n",
    "        # Keep only Volume_mm3\n",
    "        if 'Volume_mm3' in aseg_df.columns:\n",
    "            flattened[f\"aseg_{struct_name}_Volume_mm3\"] = row['Volume_mm3']\n",
    "        \n",
    "        # Add ICV once (same for all rows)\n",
    "        if idx == 0 and 'ICV' in aseg_df.columns:\n",
    "            flattened['aseg_ICV'] = row['ICV']\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "# Function to flatten aparc data - keep only GrayVol\n",
    "def flatten_aparc(aparc_df, hemisphere):\n",
    "    \"\"\"Keep only GrayVol (gray matter volume in mm3) from aparc data\"\"\"\n",
    "    flattened = {}\n",
    "    \n",
    "    for idx, row in aparc_df.iterrows():\n",
    "        region_name = row['StructName'].replace('-', '_').replace(' ', '_')\n",
    "        \n",
    "        # Keep only GrayVol (gray matter volume)\n",
    "        if 'GrayVol' in aparc_df.columns:\n",
    "            flattened[f\"{hemisphere}_{region_name}_GrayVol\"] = row['GrayVol']\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "# Collect all participant data\n",
    "all_data = []\n",
    "errors = 0\n",
    "\n",
    "for participant_dir in tqdm(participant_dirs, desc=\"Processing participants\"):\n",
    "    try:\n",
    "        participant_id, visit = extract_participant_info(participant_dir.name)\n",
    "        stats_folder = participant_dir / \"stats\"\n",
    "        \n",
    "        # Define CSV file paths\n",
    "        aseg_csv = stats_folder / \"aseg_parsed.csv\"\n",
    "        lh_aparc_csv = stats_folder / \"lh.aparc_parsed.csv\"\n",
    "        rh_aparc_csv = stats_folder / \"rh.aparc_parsed.csv\"\n",
    "        \n",
    "        # Load CSVs\n",
    "        aseg_df = pd.read_csv(aseg_csv)\n",
    "        lh_aparc_df = pd.read_csv(lh_aparc_csv)\n",
    "        rh_aparc_df = pd.read_csv(rh_aparc_csv)\n",
    "        \n",
    "        # Start with participant info\n",
    "        row_data = {\n",
    "            'participant_id': participant_id,\n",
    "            'visit': visit\n",
    "        }\n",
    "        \n",
    "        # Add ASEG Volume data (45 structures with Volume_mm3 + ICV)\n",
    "        aseg_flat = flatten_aseg(aseg_df)\n",
    "        row_data.update(aseg_flat)\n",
    "        \n",
    "        # Add LH and RH APARC GrayVol data (34 regions × GrayVol each hemisphere)\n",
    "        lh_flat = flatten_aparc(lh_aparc_df, 'lh')\n",
    "        rh_flat = flatten_aparc(rh_aparc_df, 'rh')\n",
    "        row_data.update(lh_flat)\n",
    "        row_data.update(rh_flat)\n",
    "        \n",
    "        all_data.append(row_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print(f\"✗ Error processing {participant_dir.name}: {e}\")\n",
    "\n",
    "# Create combined DataFrame\n",
    "if all_data:\n",
    "    combined_df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Save to T1 folder\n",
    "    output_path = t1_folder / \"all_participants_freesurfer_data.csv\"\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Combined data saved!\")\n",
    "    print(f\"  Participants: {len(combined_df)}\")\n",
    "    print(f\"  Total Variables: {len(combined_df.columns)}\")\n",
    "    print(f\"  Output: {output_path}\")\n",
    "    print(f\"\\nVariable breakdown:\")\n",
    "    print(f\"  - Base columns (participant_id, visit): 2\")\n",
    "    print(f\"  - ASEG volumes (45 structures × Volume_mm3 + ICV): 46\")\n",
    "    print(f\"  - LH APARC volumes (34 regions × GrayVol): 34\")\n",
    "    print(f\"  - RH APARC volumes (34 regions × GrayVol): 34\")\n",
    "    print(f\"  - TOTAL: {len(combined_df.columns)}\")\n",
    "    print(f\"\\nSample columns:\")\n",
    "    print(combined_df.columns.tolist()[:20])\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(combined_df.iloc[:3, :6])\n",
    "    print(f\"\\nErrors: {errors}\")\n",
    "else:\n",
    "    print(\"✗ No data to combine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
